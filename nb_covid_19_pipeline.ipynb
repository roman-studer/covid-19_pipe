{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC - Covid-19 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task description: https://ds-spaces.technik.fhnw.ch/app/uploads/sites/82/2020/09/minichallenge_covid19.pdf\n",
    "\n",
    "Author: Roman Janic Studer\n",
    "\n",
    "Data Pipeline Concept: https://en.wikipedia.org/wiki/Pipeline_(computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [x] Create function for daily data pull\n",
    "2. [x] Drop unneccesary data\n",
    "3. [x] Clean data (tidy data principe)\n",
    "4. [X] Prepare data for visualization (aggregation maybe)\n",
    "5. [x] Return global and local dataframe\n",
    "6. [x] Create visualization (plotly) for global and local data (barplots incl. moving average e. g. srf.ch and linecharts)\n",
    "7. [x] Document process and code\n",
    "\n",
    "Steps 1 to 5 should function as a data preparation pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Zahlen, die täglich pro Land geliefert werden sollen:\n",
    "\n",
    "[x] total bestätigte Fälle pro Land seit Anfang der Epidemie\n",
    "[x] neue bestätigte Fälle pro Land seit Anfang der Epidemie\n",
    "[x] total bestätigte Todesfälle pro Land seit Anfang der Epidemie\n",
    "[x] neue bestätigte Todesfälle pro Land seit Anfang der Epidemie\n",
    "\n",
    "Zahlen, die täglich pro Kanton geliefert werden sollen:\n",
    "\n",
    "[x] total bestätigte Fälle pro Kanton seit 1. Juni 2020 (Beginn 2. Welle)\n",
    "[x] neue bestätigte Fälle pro Kanton seit 1. Juni 2020\n",
    "[x] total bestätigte Todesfälle pro Kanton seit 1. Juni 2020\n",
    "[x] neue bestätigte Todesfälle pro Kanton seit 1. Juni 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasource local = https://github.com/daenuprobst/covid19-cases-switzerland\n",
    "# Datasource global = https://github.com/CSSEGISandData/COVID-19\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_data()\n",
    "This function pulls 4 dataframes from two github repositories. Two dataframes from the John Hopkins University containing the daily total cases and total fatalities of Covid-19 from every country, and two from the covid19-cases-switzerland repository from David Probst containing daily total cases and total fatalities of Covid-19 from every Canton in Switzerland. The function utilises the `pandas.read_csv()` function and pulls the data directly via an URL to the raw data. It returns a List of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Pulls latest data from sources\n",
    "    :return df_global: Dataframe containg current Covid-19 Data from John Hopkins University\n",
    "    :return df_CH_cases: Daily new cases in Switzerland\n",
    "    :return df_CH_fatal: Daily new fatalities in Switzerland\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get most recent data from john hopkins\n",
    "    df_global_daily_cases = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')    \n",
    "    df_global_daily_fatal = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "    \n",
    "    # Get most recent data from covid19-cases-switzerland\n",
    "    df_CH_cases = pd.read_csv('https://raw.githubusercontent.com/daenuprobst/covid19-cases-switzerland/master/covid19_cases_switzerland_openzh-phase2.csv')\n",
    "    df_CH_fatal = pd.read_csv('https://raw.githubusercontent.com/daenuprobst/covid19-cases-switzerland/master/covid19_fatalities_switzerland_openzh-phase2.csv')\n",
    "        \n",
    "    return [df_global_daily_cases, df_global_daily_fatal, df_CH_cases, df_CH_fatal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop_columns()\n",
    "Muliple columns are not necessary for the data visualization and get dropped with `pandas.DataFrame.drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(dfs):\n",
    "    \"\"\"\n",
    "    Drops columns in list of dataframes if column name is in columns\n",
    "    :param dfs: Lisst of dataframes\n",
    "    :return dfs: List of dataframes\n",
    "    \"\"\"\n",
    "    DROP_COLUMNS = ['FIPS','Admin2','Province_State','Recovered','Combined_Key',\n",
    "                'Incidence_Rate','Case-Fatality_Ratio', 'Province/State', 'Last_Update', 'Long','Lat']\n",
    "    \n",
    "    for df in dfs:\n",
    "        columns = df.columns.intersection(DROP_COLUMNS) # get list of columns to drop\n",
    "        df.drop(columns=columns, inplace=True)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename()\n",
    "To get more uniform data and therefore less complication in later processes, it is useful to rename columns to have uniform names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(dfs):\n",
    "    \"\"\"\n",
    "    Rename Columns with same information to get uniform columns across dataframes (easier down the line)\n",
    "    :param dfs: List of dataframes\n",
    "    :return dfs: List of dataframes with equal column names\n",
    "    \"\"\"\n",
    "    for df in dfs:\n",
    "        if 'Country_Region' in df.columns or 'Country/Region' in df.columns:\n",
    "            df.rename({'Country_Region':'country',\n",
    "                       'Country/Region':'country'}, inplace=True, axis=1)\n",
    "        if 'Date' in df.columns:\n",
    "            df.rename({'Date':'date'}, inplace=True, axis=1)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop_nan()\n",
    "Drops empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan(dfs):\n",
    "    \"\"\"\n",
    "    Drops empty rows\n",
    "    :param dfs: Lisst of dataframes\n",
    "    :return dfs: List of dataframes without NAN rows\n",
    "    \"\"\"\n",
    "    for df in dfs:\n",
    "        df.dropna(how='all', inplace=True)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupby_country()\n",
    "The two DataFrames of John Hopkins University have countries that occur several times because they have been divided into provinces/states. For example, the USA occurs several times because its numbers are given per state. These numbers can be summed up to get only one line for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_country(dfs):\n",
    "    \"\"\"\n",
    "    Group data by Country or. Canton\n",
    "    :param dfs: List of dataframes\n",
    "    :return dfs: List of dataframes all grouped by country or Canton\n",
    "    \"\"\"\n",
    "    for i,df in enumerate(dfs):\n",
    "        # World data\n",
    "        if 'country' in df.columns:\n",
    "            df_copy = df.copy()\n",
    "            df_copy = df_copy[['country']]\n",
    "            \n",
    "            # get all columns which should be summed up. I.e. all collumns with case numbers\n",
    "            columns = [e for e in df.columns if e not in ['country']] \n",
    "            \n",
    "            # group by country and sum up the case numbers\n",
    "            df = df.groupby(by=['country'])[columns].agg('sum').reset_index() \n",
    "            \n",
    "            # merge on original list\n",
    "            df = df.merge(df_copy, left_on='country', right_on='country') \n",
    "            \n",
    "            # drop duplicated countries\n",
    "            dfs[i] = df.drop_duplicates(subset='country', keep='first') \n",
    "            \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## melt_data()\n",
    "This function is the heart of the pipeline. It melts the Dataframes and calculates the new columns.\n",
    "To avoid problems with the calculation of the new daily cases (it can happen that data from one country is combined with data from another country), the DataFrames are divided per country/canton, the new data is calculated and finally merged into one DataFrame.\n",
    "\n",
    "`pandas.DataFrame.melt()` led to some problems with the John Hopkins Data (calculation of \"new_cases/fatal\" led to negativ numbers, which should not be possible). Another approach was used as a workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_data(dfs):\n",
    "    \"\"\"\n",
    "    Melts DataFrames into format \"country\"(\"canton\"),\"date\",\"total_cases\",\"new_cases\",\"total_fatal\",\"new_fatal\"\n",
    "    :param dfs: List of Dataframes\n",
    "    :return dfs: List of Dataframes in new format\n",
    "    \"\"\"\n",
    "    label = ['cases','fatal','cases','fatal']\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        \n",
    "        # global data\n",
    "        if 'country' in df.columns:\n",
    "            df_new = None\n",
    "            for c in df.country.unique():\n",
    "                df_c = df[df['country']==c].copy()\n",
    "                \n",
    "                # transpose DataFrame\n",
    "                df_c  = df_c.T.reset_index() \n",
    "                \n",
    "                # clean dataset and rename columns\n",
    "                df_c.columns = ['date',f'total_{label[i]}']\n",
    "                df_c = df_c.drop(0)\n",
    "                \n",
    "                # calculate new columns\n",
    "                df_c[f'new_{label[i]}'] = df_c[f'total_{label[i]}'].diff() # new_cases/fatal column\n",
    "                \n",
    "                df_c['country'] = c\n",
    "                \n",
    "                if df_new is None:\n",
    "                    df_new = df_c\n",
    "                else:\n",
    "                    df_new = df_new.append(other=df_c,ignore_index=True)\n",
    "                    \n",
    "            # add new_data\n",
    "            dfs[i] = df_new\n",
    "            \n",
    "        # local data\n",
    "        else:\n",
    "            df_new = None\n",
    "            # melt dataframe\n",
    "            df = df.melt(id_vars='date',var_name='canton',value_name=f'total_{label[i]}')\n",
    "            for c in df.canton.unique():\n",
    "                df_c = df[df['canton']==c].copy()\n",
    "                \n",
    "                # calculate new columns\n",
    "                df_c[f'new_{label[i]}'] = df_c[f'total_{label[i]}'].diff() # new_cases/fatal column\n",
    "                \n",
    "                if df_new is None:\n",
    "                    df_new = df_c\n",
    "                else:\n",
    "                    df_new = df_new.append(other=df_c,ignore_index=True)\n",
    "            \n",
    "            # add new_data\n",
    "            dfs[i] = df_new\n",
    "            \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_data()\n",
    "This function merges the four DataFrames into df_global and df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(dfs):\n",
    "    \"\"\"\n",
    "    Merges prepared Dataframes into two dataframes (df_global and df_local)\n",
    "    :param dfs: List of DataFrames\n",
    "    :return df_global: DataFrame containing Covid-Data for every country\n",
    "    :return df_local: DataFrame containing Covid-Data for every canton in switzerland\n",
    "    \"\"\"\n",
    "    df_global, df_local = None, None\n",
    "    \n",
    "    for df in dfs:\n",
    "        if 'country' in df.columns:\n",
    "            if df_global is None:\n",
    "                df_global = df\n",
    "            else:\n",
    "                df_global = df_global.merge(right=df, on=['country','date'],how='left')\n",
    "        elif 'canton' in df.columns:\n",
    "            if df_local is None:\n",
    "                df_local = df\n",
    "            else:\n",
    "                df_local = df_local.merge(right=df, on=['canton','date'],how='left')\n",
    "\n",
    "    return df_global, df_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## covid_pipe()\n",
    "Strings all the above mentioned functions together to form a pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_pipe():\n",
    "    \"\"\"\n",
    "    Covid-19 data pipeline\n",
    "    :return df_global: DataFrame containing total_cases, new_cases, total_fatal (fatalities), \n",
    "                       new_fatal for every day and country since tracking\n",
    "    :return df_local: DataFrame containing total_cases, new_cases, total_fatal (fatalities), \n",
    "                      new_fatal for every day and canton in switzerland since tracking\n",
    "    \"\"\"\n",
    "    dfs = get_data()\n",
    "    dfs = drop_columns(dfs)\n",
    "    dfs = rename(dfs)\n",
    "    dfs = drop_nan(dfs)\n",
    "    dfs = groupby_country(dfs)\n",
    "\n",
    "    dfs = melt_data(dfs)\n",
    "    df_global, df_local = merge_data(dfs)\n",
    "    \n",
    "    return df_global, df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global, df_local = covid_pipe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>canton</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_fatal</th>\n",
       "      <th>new_fatal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>AG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-02</td>\n",
       "      <td>AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-06-03</td>\n",
       "      <td>AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>AG</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date canton  total_cases  new_cases  total_fatal  new_fatal\n",
       "0  2020-05-31     AG          NaN        NaN          NaN        NaN\n",
       "1  2020-06-01     AG          3.0        NaN          NaN        NaN\n",
       "2  2020-06-02     AG          3.0        0.0          0.0        NaN\n",
       "3  2020-06-03     AG          4.0        1.0          NaN        NaN\n",
       "4  2020-06-04     AG          6.0        2.0          0.0        NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_local.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
